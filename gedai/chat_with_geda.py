#!/usr/bin/env python3
# chat_with_geda.py - AI-assisted interactive schematic design interface
#
# Copyright (C) 2025 John Ryan, maintainer geda-ai
#
# This file is part of gEDA-AI.
#
# gEDA-AI is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 2 of the License, or
# (at your option) any later version.
#
# gEDA-AI is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with gEDA-AI.  If not, see <https://www.gnu.org/licenses/>.
# TODO: When gEDA-AI is fully running, implement integration with gschem.
# Specifically, send a signal or trigger a re-read of the .schem file
# so that changes generated by chat_with_geda.py are reflected live.
import os
import sys
import json
import readline
import requests
from datetime import datetime
from pathlib import Path
import tkinter as tk
from tkinter import simpledialog, messagebox, scrolledtext

OLLAMA_ENDPOINT = os.getenv("OLLAMA_API", "http://localhost:11434/api/chat")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "mistral")

SYSTEM_PROMPT = """
You are an AI assistant integrated with the gEDA-AI suite.
You help users design electronic circuits by generating .sch files, BOMs,
and suggestions in plain text based on their natural language input.
You respond using Markdown and code blocks where appropriate.
If your reply includes gEDA schematic or BOM data, format clearly for export.
"""

SAVE_DIR = Path("output")
SAVE_DIR.mkdir(exist_ok=True)


def send_query_to_ollama(prompt, history):
    payload = {
        "model": OLLAMA_MODEL,
        "messages": history + [{"role": "user", "content": prompt}],
        "stream": False
    }
    try:
        response = requests.post(OLLAMA_ENDPOINT, json=payload)
        response.raise_for_status()
        data = response.json()
        return data.get("message", {}).get("content", "[No response from model]")
    except Exception as e:
        return f"[Error: {e}]"


def extract_and_save(content, prefix):
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    base_path = SAVE_DIR / f"{prefix}_{timestamp}"

    sch_lines = []
    bom_lines = []
    md_lines = []
    current_block = []
    mode = None

    for line in content.splitlines():
        if line.startswith("```"):
            if mode == "sch":
                (base_path.with_suffix(".sch")).write_text("\n".join(current_block))
            elif mode == "bom":
                (base_path.with_suffix(".bom")).write_text("\n".join(current_block))
            current_block = []
            mode = None
            continue
        elif line.strip().startswith(".schematic"):
            mode = "sch"
        elif "BOM" in line.upper() or line.lower().startswith("| part"):
            mode = "bom"
        elif mode:
            current_block.append(line)
        else:
            md_lines.append(line)

    (base_path.with_suffix(".md")).write_text("\n".join(md_lines))
    return base_path


class ChatGUI:
    def __init__(self):
        self.window = tk.Tk()
        self.window.title("gEDA-AI Assistant")

        self.text_area = scrolledtext.ScrolledText(self.window, wrap=tk.WORD, width=80, height=30)
        self.text_area.pack(padx=10, pady=10)

        self.entry = tk.Entry(self.window, width=80)
        self.entry.bind("<Return>", self.submit_query)
        self.entry.pack(padx=10, pady=5)

        self.history = [{"role": "system", "content": SYSTEM_PROMPT}]

    def submit_query(self, event=None):
        user_input = self.entry.get().strip()
        if not user_input:
            return

        self.text_area.insert(tk.END, f">> {user_input}\n")
        self.entry.delete(0, tk.END)
        self.text_area.insert(tk.END, "[Thinking...]\n")
        self.text_area.update()

        response = send_query_to_ollama(user_input, self.history)
        self.text_area.insert(tk.END, f"{response}\n{'='*40}\n")
        self.text_area.see(tk.END)

        self.history.append({"role": "user", "content": user_input})
        self.history.append({"role": "assistant", "content": response})

        base_file = extract_and_save(response, prefix="geda_ai_response")
        messagebox.showinfo("Saved", f"Saved response files to: {base_file.with_suffix('.*')}")

    def run(self):
        self.window.mainloop()


if __name__ == "__main__":
    if "--gui" in sys.argv:
        ChatGUI().run()
    else:
        print("=== gEDA-AI Chat Assistant ===")
        print("Describe your desired circuit (e.g. 'Create a DO sensor with TMP chip and give me a .sch')\n")

        history = [{"role": "system", "content": SYSTEM_PROMPT}]

        while True:
            try:
                prompt = input(">> ").strip()
                if prompt.lower() in ("exit", "quit"):
                    print("Exiting...")
                    break
                if not prompt:
                    continue

                print("\n[Thinking...]\n")
                reply = send_query_to_ollama(prompt, history)
                print(reply)
                print("\n" + "=" * 40 + "\n")

                history.append({"role": "user", "content": prompt})
                history.append({"role": "assistant", "content": reply})

                extract_and_save(reply, prefix="geda_ai_response")

            except KeyboardInterrupt:
                print("\nInterrupted. Type 'quit' to exit.")
            except Exception as e:
                print(f"[Exception: {e}]")
